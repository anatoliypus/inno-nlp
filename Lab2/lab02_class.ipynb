{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1407d943-d2a2-4a7a-895d-dd912039941c",
   "metadata": {},
   "source": [
    "# NLP. Lab 2. Text processing basics\n",
    "\n",
    "In this lab, we will cover a wide range of NLP concepts, including Sentence Segmentation, Lowercasing, Stop Words Removal, Lemmatization, Stemming, Byte-Pair Encoding (BPE), and Edit Distance. Theoretical overviews and practical examples for each concept will be provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b5da3-110f-4c6e-8764-63cda762c57b",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "\n",
    "Sentence segmentation involves breaking down a text into individual sentences, typically separated by punctuation marks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0178922e-cba3-41e4-aa83-acdf7fa38181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample text.', 'It contains multiple sentences.', 'Can we segment it?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"This is a sample text. It contains multiple sentences. Can we segment it?\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c1aa6-6680-4ad6-aa0d-278c782580c0",
   "metadata": {},
   "source": [
    "## Lowercasing\n",
    "\n",
    "Lowercasing converts all text to lowercase, ensuring uniformity and simplifying text processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab620e1-dbf8-4e06-8966-974c54da2a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example text.\n"
     ]
    }
   ],
   "source": [
    "text = \"ThIs Is AN ExaMple Text.\"\n",
    "lowercased_text = text.lower()\n",
    "\n",
    "print(lowercased_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2fb22-edf9-4e9f-9922-e446343763b7",
   "metadata": {},
   "source": [
    "## Stop Words Removal\n",
    "\n",
    "Stop words are common words (e.g., \"the,\" \"and\") that are often removed during text processing to focus on meaningful words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27f6d73-270d-45da-8619-c8edd2b28800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'stop', 'words.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "text = \"This is an example sentence with some stop words.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d58ac-d436-45ac-953c-83d012cd4c4b",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base or dictionary form, considering the context and applying morphological analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d69808e-3b41-4082-9407-53360e8f8c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rock', 'corpus', 'cry']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"rocks\", \"corpora\", \"cries\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602576b-bc1a-4744-8632-33df9c2a16b4",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming reduces words to their stems or root form, often by removing suffixes, in a more heuristic approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af22fa2-8780-4ce9-9260-340995893579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'rock', 'beauti']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"rocks\", \"beautifully\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b62c6-1633-4f60-87c2-84797f36aa77",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE is a data compression technique used in NLP for tokenization. It breaks down words into subword units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4135cf-365f-4e28-b903-538669439f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "945b3193-5242-4e8d-a797-53dabcbd6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "temp_proc = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", special_tokens.index(\"[CLS]\")),\n",
    "        (\"[SEP]\", special_tokens.index(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5348f7ef-20f7-4aac-9fb6-f46087066338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.decoders import BPEDecoder\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.decoder = BPEDecoder()\n",
    "tokenizer.post_processor = temp_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc9e61d-1c9c-4c8d-84ec-149bd123d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5d4f76-feba-42ec-a8e3-d9f9e0c61f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "nltk.download(\"gutenberg\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "trainer = BpeTrainer(vocab_size=5000, special_tokens=special_tokens)\n",
    "shakespeare = [\" \".join(s) for s in gutenberg.sents(\"shakespeare-macbeth.txt\")]\n",
    "tokenizer.train_from_iterator(shakespeare, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea8efe49-2cbe-4bf0-8f91-f6b84f1570f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'b', 'pe', 'is', 'a', 'd', 'at', 'a', 'com', 'pre', 'ss', 'ion', 'te', 'ch', 'ni', 'que', 'use', 'd', 'in', 'n', 'lp', 'for', 'to', 'ken', 'iz', 'ation', '.', '[SEP]']\n",
      "['[CLS]', 'is', 'this', 'a', 'danger', 'which', 'i', 'see', 'before', 'me', ',', 'the', 'handle', 'toward', 'my', 'hand', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.encode(\n",
    "        \"BPE is a data compression technique used in NLP for tokenization.\"\n",
    "    ).tokens\n",
    ")\n",
    "print(\n",
    "    tokenizer.encode(\n",
    "        \"Is this a danger which I see before me, the handle toward my hand?\"\n",
    "    ).tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22796e-92f0-4d6b-85c0-eca37e819ee3",
   "metadata": {},
   "source": [
    "## Levenshtein edit distance\n",
    "\n",
    "Edit distance measures the similarity between two strings by counting the minimum number of operations needed to transform one string into the other.\n",
    "\n",
    "[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance#Example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6969f720-4edf-4dfb-9fb9-8c49561cc901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76bc7092-c2be-4324-b674-2ae1e6a91b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance between 'kitten' and 'sitting': 3\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "word1 = \"kitten\"\n",
    "word2 = \"sitting\"\n",
    "distance = Levenshtein.distance(word1, word2)\n",
    "print(f\"Edit distance between '{word1}' and '{word2}': {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a169b-7437-43fe-9bb7-f705037289f4",
   "metadata": {},
   "source": [
    "# Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9fb6a-d744-4950-a9de-9b03febf7e70",
   "metadata": {},
   "source": [
    "[Competition](https://www.kaggle.com/t/6dcb6f9def724f9f82050e9092952dd6)\n",
    "\n",
    "The aim of the competition is to count the 10 most frequent words in the plays presented in the `data.txt` file.\n",
    "\n",
    "In order to count the frequent words correctly, you must perform lemmatization and remove stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa7c6461-6389-4c36-be91-63c493ee1e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-caesar.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data.txt\") as f:\n",
    "    data = f.read()\n",
    "plays = data.split(\"\\n\")\n",
    "plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b669c3d-5f63-460c-af95-ff71bc42f597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt 887071\n",
      "austen-persuasion.txt 466292\n",
      "austen-sense.txt 673022\n",
      "shakespeare-macbeth.txt 100351\n",
      "shakespeare-hamlet.txt 162881\n",
      "shakespeare-caesar.txt 112310\n"
     ]
    }
   ],
   "source": [
    "plays_dict = {}\n",
    "\n",
    "for play in plays:\n",
    "    plays_dict[play] = gutenberg.raw(play)\n",
    "    print(play, len(plays_dict[play]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bababbbd-c25e-4c02-bcfd-3dcb69f107b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from collections import Counter\n",
    "\n",
    "def top_frequent_words(text, topk=10):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    cnt = Counter(lemmatized_words)\n",
    "    most_common = cnt.most_common(topk)\n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2f4d7ab-915d-4b6b-a537-6373f47e8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = {}\n",
    "for play, text in plays_dict.items():\n",
    "    top_words[play] = top_frequent_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5e1d600-f6cb-4df8-9522-52810cd349bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"id,count\\n\")\n",
    "    for play, counts in top_words.items():\n",
    "        for i, count in enumerate(counts):\n",
    "            f.write(f\"{play}_{i},{count[1]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
