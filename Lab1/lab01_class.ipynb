{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2edd30-3ef7-421f-a4a3-d406610b7b3d",
   "metadata": {},
   "source": [
    "# NLP. Lab 1. Tokenization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eae649-8e0a-45e0-b353-3f4ea6a6d422",
   "metadata": {},
   "source": [
    "## What is tokenization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354d7d4-3728-4636-b876-1ae25ca1e795",
   "metadata": {},
   "source": [
    "Tokenization is one of the first step in any NLP pipeline. Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens. If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'. Generally 'space' is used to perform the word tokenization and characters like 'periods, exclamation point and newline char are used for Sentence Tokenization. We have to choose the appropriate method as per the task in hand. While performing the tokenization few characters like spaces, punctuations are ignored and will not be the part of final list of tokens.\n",
    "\n",
    "![NLP_Tokenization](https://raw.githubusercontent.com/satishgunjal/images/master/NLP_Tokenization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c749d-fb39-4747-8c3e-43f88b7013b3",
   "metadata": {},
   "source": [
    "### Purpose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7ec80-d1a4-4823-9c26-849e5289a9f8",
   "metadata": {},
   "source": [
    "Every sentence gets its meaning by the words present in it. So by analyzing the words present in the text we can easily interpret the meaning of the text. Once we have a list of words we can also use statistical tools and methods to get more insights into the text. For example, we can use word count and word frequency to find out important of word in that sentence or document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9b8fc-4976-41f4-97d9-2464cd50b122",
   "metadata": {},
   "source": [
    "## Tokenization in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260412eb-3fc2-465a-aa40-8d9f9221fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenization is one of the first step in any NLP pipeline. Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens. If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'. Generally 'space' is used to perform the word tokenization and characters like 'periods, exclamation point and newline char are used for Sentence Tokenization.  We have to choose the appropriate method as per the task in hand. While performing the tokenization few characters like spaces, punctuations are ignored and will not be the part of final list of tokens.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee832e5-6f35-4f36-9adb-4016f4290bb4",
   "metadata": {},
   "source": [
    "### Built-in methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3eb88-9617-488c-8df8-3e185b7acb53",
   "metadata": {},
   "source": [
    "We can use **split()** method to split a string into a list where each word is a list item.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113b99f-7fe1-4431-872e-41cb42ea2d03",
   "metadata": {},
   "source": [
    "#### Word tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf492d98-e69b-4164-ab15-5da3eb3f620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'one', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "print(tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba184a73-0600-4eb9-9653-1b2f5ad88a6c",
   "metadata": {},
   "source": [
    "#### Sentence tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94809ca6-daff-4c98-ae0d-a739c5b4096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is one of the first step in any NLP pipeline', ' Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens', \" If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'\"]\n"
     ]
    }
   ],
   "source": [
    "tokens = text.split(\".\")\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b238e-1d54-49e5-80f1-e995cd5dbe88",
   "metadata": {},
   "source": [
    "### RegEx tokenization\n",
    "\n",
    "- Using RegEx we can match character combinations in string and perform word/sentence tokenization.\n",
    "- You can check your regular expressions at [regex101](https://regex101.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e4c3f-bdd2-487d-9103-4f53bc65d78e",
   "metadata": {},
   "source": [
    "#### Word tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe23141-8fb7-4847-837a-b8c9919b3329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'one', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.findall(\"[\\w]+\", text)\n",
    "print(tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75296723-818a-4f79-bf19-2f6667d6bbc5",
   "metadata": {},
   "source": [
    "### NLTK library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd0a61-4237-4e76-bdb9-64300e258004",
   "metadata": {},
   "source": [
    "#### Word tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5697f541-8d47-405b-93d1-088b3fd2c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac8af61d-395c-489d-ad2f-dd4d3ba1523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'one', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418269d2-db38-42f6-8c5f-ae19c16530a5",
   "metadata": {},
   "source": [
    "#### Sentence tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd2e844-f17f-44c3-8547-aea3056b2881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is one of the first step in any NLP pipeline.', 'Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens.', \"If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokens = sent_tokenize(text)\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c59f7-eea3-4032-8db8-7638122d0ff1",
   "metadata": {},
   "source": [
    "### Spacy library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff4df4-c446-4bf7-8aa7-0709e7e9fe4f",
   "metadata": {},
   "source": [
    "#### Word tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "154868b4-915c-45c0-b282-e716c4c0cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b0acd17-2b5c-49de-819a-a75384d14ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'one', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "english_tokenizer = English()\n",
    "\n",
    "doc = english_tokenizer(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b026f2-895a-4db0-bbfc-7ab8cbf5cb2f",
   "metadata": {},
   "source": [
    "#### Sentence tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80baf370-9bd4-4243-84da-8ccbf361ee57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenization is one of the first step in any NLP pipeline., Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens., If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'.]\n"
     ]
    }
   ],
   "source": [
    "english_tokenizer = English()\n",
    "english_tokenizer.add_pipe(\"sentencizer\")\n",
    "\n",
    "\n",
    "doc = english_tokenizer(text)\n",
    "tokens = [token.sent for token in doc.sents]\n",
    "print(tokens[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828e650-0fc3-4365-95d2-07a76e5dbb34",
   "metadata": {},
   "source": [
    "## Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa023e4-485a-4335-834e-33aa46a1ec80",
   "metadata": {},
   "source": [
    "Your goal is to solve tokenization task and count number of numeric tokens.\n",
    "\n",
    "You should submit your solution to [competition](https://www.kaggle.com/t/50b3669520ce4a0e892900406bbc1f2f).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a11c1-1478-477f-b622-907a262d3c8b",
   "metadata": {},
   "source": [
    "### Grade distribution\n",
    "\n",
    "- your solution is ranked above or the same as the benchmark solution in the leaderboard - 1 point\n",
    "- your solution is lower than the benchmark solution - 0.5 points\n",
    "- no submission / late submission / no appearance on leaderboard - 0 points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86e09c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 1, 3, 6, 2, 1, 3, 2, 1, 1, 2, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "counts = list()\n",
    "\n",
    "with open('./data.txt', 'r') as f:\n",
    "    for id, s in enumerate(f.readlines()):\n",
    "        counts.append(len(re.findall(r'\\d+', s)))\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85f222c2-b0d0-400f-b0d5-4bb92ef8e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"id,count\\n\")\n",
    "    for id, count in enumerate(counts):\n",
    "        f.write(f\"{id},{count}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
